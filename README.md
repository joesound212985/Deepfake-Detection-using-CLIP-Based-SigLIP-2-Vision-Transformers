Deepfakes pose a growing threat to visual media integrity, necessitating robust detection methods. This paper proposes a deepfake detection approach based on CLIP-derived vi-sion transformers (SigLIP-2), combined with a multi-task design for classification and manipulated region localization. The models are evaluated on three public benchmarks of increasing complexity: HiDF, a recent “human-indistinguishable” deepfake video/image dataset, SIDA, a large social-media image dataset with pixel-level annotations and expla-nations, and CIFake, a controlled CIFAR-10-based fake image benchmark. Our detector achieves state-of-the-art results on all three. On HiDF, it attains an AUC of 0.931 for deep-fake video detection – improving ~0.20 over the best prior (EB4) – and similarly high 0.968 AUC on images. On SIDA, the model reaches 99.1% accuracy, substantially outperforming the previous 93.5% baseline while correctly localizing most tampered pixels. It also ex-ceeds 95% accuracy on CiFake, with an AUC of 0.986. These results demonstrate the effec-tiveness of large-scale pre-trained vision transformers for deepfake forensics. The pro-posed model substantially advances detection performance on challenging realistic for-geries, providing both high precision and interpretable localization to support practical deepfake mitigation
