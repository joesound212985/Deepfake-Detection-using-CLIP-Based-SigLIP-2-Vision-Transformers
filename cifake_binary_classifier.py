#!/usr/bin/env python3
"""
Clean CIFAKE Binary Classifier
Real vs Fake image classification using SigLIP vision transformer
"""

import os
import torch

# Memory optimization for giant model
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Enable speed optimizations early
import torch.backends.cudnn as cudnn
torch.set_float32_matmul_precision('high')  # allow TF32
cudnn.benchmark = True
try:
    torch.backends.cuda.matmul.allow_tf32 = True
    cudnn.allow_tf32 = True
    # Prefer Flash/Mem-efficient SDPA when available
    if hasattr(torch.backends.cuda, "enable_flash_sdp"):
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        torch.backends.cuda.enable_math_sdp(False)
except Exception:
    pass

# Import dynamo at top level to avoid shadowing issues
try:
    import torch._dynamo
    DYNAMO_AVAILABLE = True
except ImportError:
    DYNAMO_AVAILABLE = False
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from PIL import Image
import numpy as np
from io import BytesIO
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, confusion_matrix, cohen_kappa_score
from sklearn.calibration import calibration_curve
from sklearn.utils import resample
from scipy import stats
from scipy.stats import chi2_contingency, fisher_exact
import pandas as pd
import argparse
from tqdm import tqdm
import json
import kornia
import kornia.augmentation as K
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.animation import FuncAnimation
from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score
import time

try:
    import open_clip
    OPENCLIP_AVAILABLE = True
except ImportError:
    OPENCLIP_AVAILABLE = False
    print("Warning: open_clip not available. Install with: pip install open-clip-torch")

# Advanced augmentation imports
try:
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    ALBUMENTATIONS_AVAILABLE = True
except ImportError:
    ALBUMENTATIONS_AVAILABLE = False
    print("Warning: albumentations not available. Install with: pip install albumentations")

# Using OpenCLIP SigLIP models only

# FSDP imports
try:
    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
    from torch.distributed.fsdp import MixedPrecision, BackwardPrefetch, StateDictType
    from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
    FSDP_AVAILABLE = True
except ImportError:
    FSDP_AVAILABLE = False

class UltraJPEGTransform:
    """Ultra-aggressive JPEG compression for robustness training"""
    def __init__(self, quality_range=(5, 25), probability=0.3):
        self.quality_range = quality_range
        self.probability = probability
    
    def __call__(self, image):
        if np.random.random() < self.probability:
            # Convert PIL to bytes with ultra-low JPEG quality
            quality = np.random.randint(self.quality_range[0], self.quality_range[1] + 1)
            
            # Save to bytes buffer with ultra compression
            buffer = BytesIO()
            # Convert RGBA to RGB if needed
            if image.mode == 'RGBA':
                image = image.convert('RGB')
            image.save(buffer, format='JPEG', quality=quality, optimize=True)
            
            # Reload from compressed bytes
            buffer.seek(0)
            compressed_image = Image.open(buffer).convert('RGB')
            buffer.close()
            
            return compressed_image
        return image

class CIFAKEDataset(Dataset):
    def __init__(self, data_dir, split='train', transform=None, use_albumentations=False, progressive_resize=None, use_ultra_jpeg=False):
        self.data_dir = data_dir
        self.split = split
        self.transform = transform
        self.use_albumentations = use_albumentations and ALBUMENTATIONS_AVAILABLE
        self.progressive_resize = progressive_resize
        self.use_ultra_jpeg = use_ultra_jpeg
        
        # Advanced augmentations for training
        if self.use_albumentations and split == 'train':
            self.album_transform = A.Compose([
                A.RandomResizedCrop(512, 512, scale=(0.8, 1.0), ratio=(0.75, 1.33)),
                A.HorizontalFlip(p=0.5),
                A.OneOf([
                    A.MotionBlur(blur_limit=3, p=0.2),
                    A.GaussianBlur(blur_limit=3, p=0.2),
                    A.Blur(blur_limit=3, p=0.1),
                ], p=0.3),
                A.OneOf([
                    A.CLAHE(clip_limit=2),
                    A.Sharpen(),
                    A.Emboss(),
                    A.RandomBrightnessContrast(),
                ], p=0.4),
                A.OneOf([
                    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),
                    A.RandomGamma(),
                    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                ], p=0.3),
                A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.2),
                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
                ToTensorV2()
            ])
        else:
            self.album_transform = None
            
        # Ultra JPEG compression for robustness
        if self.use_ultra_jpeg and split == 'train':
            # Default values - will be updated if args are available
            quality_min = getattr(self, 'jpeg_quality_min', 5)
            quality_max = getattr(self, 'jpeg_quality_max', 25) 
            probability = getattr(self, 'jpeg_probability', 0.3)
            self.ultra_jpeg = UltraJPEGTransform(quality_range=(quality_min, quality_max), probability=probability)
            print(f"ðŸ”¥ Ultra JPEG compression enabled ({quality_min}-{quality_max}% quality, {probability*100:.0f}% probability)")
        else:
            self.ultra_jpeg = None
        
        # Load from REAL/FAKE folders
        real_dir = os.path.join(data_dir, split.upper(), 'REAL')
        fake_dir = os.path.join(data_dir, split.upper(), 'FAKE')
        
        self.samples = []
        self.labels = []
        
        # Load REAL images (label=0)
        if os.path.exists(real_dir):
            for img_file in os.listdir(real_dir):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.samples.append(os.path.join(real_dir, img_file))
                    self.labels.append(0)
        
        # Load FAKE images (label=1) 
        if os.path.exists(fake_dir):
            for img_file in os.listdir(fake_dir):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.samples.append(os.path.join(fake_dir, img_file))
                    self.labels.append(1)
        
        print(f"Loaded {len(self.samples)} {split} samples: {sum(self.labels)} fake, {len(self.labels) - sum(self.labels)} real")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path = self.samples[idx]
        label = self.labels[idx]
        
        image = Image.open(img_path).convert('RGB')
        
        # Apply ultra JPEG compression first (before other transforms)
        if self.ultra_jpeg and self.split == 'train':
            image = self.ultra_jpeg(image)
        
        # Progressive resizing
        if self.progressive_resize and self.split == 'train':
            size = self.progressive_resize
            image = image.resize((size, size), Image.Resampling.LANCZOS)
        
        if self.use_albumentations and self.album_transform:
            # Convert PIL to numpy for albumentations
            image_np = np.array(image)
            augmented = self.album_transform(image=image_np)
            image = augmented['image']
        elif self.transform:
            image = self.transform(image)
        
        return image, label

class ExponentialMovingAverage:
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.shadow[name] * self.decay + param.data * (1 - self.decay)
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]

class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, pos_weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight
    
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(
            inputs, targets, pos_weight=self.pos_weight, reduction='none'
        )
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return focal_loss.mean()

class DropoutScheduler:
    """Adaptive dropout scheduler to prevent overfitting"""
    def __init__(self, model, initial_dropout=0.1, max_dropout=0.5, patience=3):
        self.model = model
        self.initial_dropout = initial_dropout
        self.max_dropout = max_dropout
        self.patience = patience
        self.current_dropout = initial_dropout
        self.best_val_loss = float('inf')
        self.wait = 0
        
    def step(self, val_loss):
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.wait = 0
            # Reduce dropout when improving
            self.current_dropout = max(self.initial_dropout, self.current_dropout * 0.95)
        else:
            self.wait += 1
            if self.wait >= self.patience:
                # Increase dropout when overfitting
                self.current_dropout = min(self.max_dropout, self.current_dropout * 1.1)
                self.wait = 0
        
        # Update model dropout
        self._update_model_dropout()
        return self.current_dropout
    
    def _update_model_dropout(self):
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = self.current_dropout

class RealTimeTrainingMonitor:
    """Real-time monitoring and plotting of training curves"""
    def __init__(self, save_dir, update_interval=1):
        self.save_dir = save_dir
        self.update_interval = update_interval
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.epochs = []
        
        # Overfitting detection
        self.overfitting_threshold = 0.05  # 5% gap threshold
        self.consecutive_overfitting = 0
        self.overfitting_alerts = []
        
        plt.ion()  # Interactive mode
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle('Real-Time Training Monitor', fontsize=16)
        
    def update(self, epoch, train_loss, val_loss, train_acc, val_acc):
        """Update training curves with new metrics"""
        self.epochs.append(epoch)
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accs.append(train_acc)
        self.val_accs.append(val_acc)
        
        # Check for overfitting
        overfitting_detected = self._detect_overfitting(train_loss, val_loss, train_acc, val_acc)
        
        if epoch % self.update_interval == 0:
            self._update_plots()
            
        return overfitting_detected
    
    def _detect_overfitting(self, train_loss, val_loss, train_acc, val_acc):
        """Detect overfitting based on loss/accuracy gaps"""
        overfitting = False
        
        # Loss gap detection
        loss_gap = val_loss - train_loss
        acc_gap = train_acc - val_acc
        
        if loss_gap > self.overfitting_threshold and acc_gap > 2.0:  # 2% accuracy gap
            self.consecutive_overfitting += 1
            overfitting = True
            
            alert = {
                'epoch': len(self.epochs),
                'loss_gap': loss_gap,
                'acc_gap': acc_gap,
                'severity': 'WARNING' if self.consecutive_overfitting < 3 else 'CRITICAL'
            }
            self.overfitting_alerts.append(alert)
        else:
            self.consecutive_overfitting = 0
            
        return overfitting
    
    def _update_plots(self):
        """Update all plots with latest data"""
        if len(self.epochs) < 2:
            return
            
        # Clear previous plots
        for ax in self.axes.flat:
            ax.clear()
        
        # Plot 1: Loss curves
        self.axes[0, 0].plot(self.epochs, self.train_losses, 'b-', label='Train Loss', linewidth=2)
        self.axes[0, 0].plot(self.epochs, self.val_losses, 'r-', label='Val Loss', linewidth=2)
        self.axes[0, 0].set_xlabel('Epoch')
        self.axes[0, 0].set_ylabel('Loss')
        self.axes[0, 0].set_title('Training vs Validation Loss')
        self.axes[0, 0].legend()
        self.axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Accuracy curves
        self.axes[0, 1].plot(self.epochs, self.train_accs, 'b-', label='Train Acc', linewidth=2)
        self.axes[0, 1].plot(self.epochs, self.val_accs, 'r-', label='Val Acc', linewidth=2)
        self.axes[0, 1].set_xlabel('Epoch')
        self.axes[0, 1].set_ylabel('Accuracy (%)')
        self.axes[0, 1].set_title('Training vs Validation Accuracy')
        self.axes[0, 1].legend()
        self.axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Loss gap (overfitting indicator)
        loss_gaps = [val - train for val, train in zip(self.val_losses, self.train_losses)]
        self.axes[1, 0].plot(self.epochs, loss_gaps, 'orange', linewidth=2, label='Loss Gap')
        self.axes[1, 0].axhline(y=self.overfitting_threshold, color='red', linestyle='--', 
                               label=f'Overfitting Threshold ({self.overfitting_threshold})')
        self.axes[1, 0].set_xlabel('Epoch')
        self.axes[1, 0].set_ylabel('Val Loss - Train Loss')
        self.axes[1, 0].set_title('Overfitting Detection (Loss Gap)')
        self.axes[1, 0].legend()
        self.axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 4: Learning rate and regularization
        if hasattr(self, 'learning_rates') and len(self.learning_rates) > 0:
            ax4_twin = self.axes[1, 1].twinx()
            self.axes[1, 1].plot(self.epochs, self.learning_rates, 'purple', linewidth=2, label='Learning Rate')
            if hasattr(self, 'dropout_rates') and len(self.dropout_rates) > 0:
                ax4_twin.plot(self.epochs, self.dropout_rates, 'green', linewidth=2, label='Dropout Rate')
                ax4_twin.set_ylabel('Dropout Rate', color='green')
            self.axes[1, 1].set_xlabel('Epoch')
            self.axes[1, 1].set_ylabel('Learning Rate', color='purple')
            self.axes[1, 1].set_title('Learning Rate & Regularization Schedule')
        else:
            # Fallback: Accuracy gap
            acc_gaps = [train - val for train, val in zip(self.train_accs, self.val_accs)]
            self.axes[1, 1].plot(self.epochs, acc_gaps, 'green', linewidth=2, label='Accuracy Gap')
            self.axes[1, 1].axhline(y=2.0, color='red', linestyle='--', label='Warning Threshold (2%)')
            self.axes[1, 1].set_xlabel('Epoch')
            self.axes[1, 1].set_ylabel('Train Acc - Val Acc (%)')
            self.axes[1, 1].set_title('Overfitting Detection (Accuracy Gap)')
            self.axes[1, 1].legend()
            self.axes[1, 1].grid(True, alpha=0.3)
        
        # Add overfitting alerts as annotations
        for alert in self.overfitting_alerts[-3:]:  # Show last 3 alerts
            color = 'orange' if alert['severity'] == 'WARNING' else 'red'
            self.axes[0, 0].annotate(f"âš ï¸ {alert['severity']}", 
                                   xy=(alert['epoch'], self.val_losses[alert['epoch']-1]),
                                   xytext=(10, 10), textcoords='offset points',
                                   color=color, fontweight='bold')
        
        plt.tight_layout()
        plt.draw()
        plt.pause(0.01)
        
        # Save plot
        self.fig.savefig(os.path.join(self.save_dir, 'realtime_training_curves.png'), 
                        dpi=150, bbox_inches='tight')
    
    def add_lr_dropout_tracking(self, lr, dropout):
        """Add learning rate and dropout tracking"""
        if not hasattr(self, 'learning_rates'):
            self.learning_rates = []
        if not hasattr(self, 'dropout_rates'):
            self.dropout_rates = []
            
        self.learning_rates.append(lr)
        self.dropout_rates.append(dropout)
    
    def get_overfitting_summary(self):
        """Get summary of overfitting detection"""
        return {
            'total_alerts': len(self.overfitting_alerts),
            'consecutive_overfitting_epochs': self.consecutive_overfitting,
            'current_overfitting_risk': 'HIGH' if self.consecutive_overfitting >= 3 else 
                                      'MEDIUM' if self.consecutive_overfitting >= 1 else 'LOW',
            'alerts': self.overfitting_alerts
        }
    
    def save_final_curves(self):
        """Save final training curves"""
        plt.ioff()  # Turn off interactive mode
        
        # Create final publication-quality plot
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Training Summary - Loss and Accuracy Curves', fontsize=18, fontweight='bold')
        
        # Enhanced styling
        plt.style.use('seaborn-v0_8-whitegrid')
        colors = {'train': '#1f77b4', 'val': '#ff7f0e', 'gap': '#2ca02c', 'threshold': '#d62728'}
        
        # Plot 1: Loss with confidence bands
        epochs_smooth = np.linspace(1, len(self.epochs), len(self.epochs))
        axes[0, 0].plot(epochs_smooth, self.train_losses, color=colors['train'], 
                       linewidth=3, label='Training Loss', alpha=0.8)
        axes[0, 0].plot(epochs_smooth, self.val_losses, color=colors['val'], 
                       linewidth=3, label='Validation Loss', alpha=0.8)
        axes[0, 0].fill_between(epochs_smooth, self.train_losses, alpha=0.2, color=colors['train'])
        axes[0, 0].fill_between(epochs_smooth, self.val_losses, alpha=0.2, color=colors['val'])
        axes[0, 0].set_xlabel('Epoch', fontsize=14)
        axes[0, 0].set_ylabel('Loss', fontsize=14)
        axes[0, 0].set_title('Training vs Validation Loss', fontsize=16, fontweight='bold')
        axes[0, 0].legend(fontsize=12)
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Accuracy
        axes[0, 1].plot(epochs_smooth, self.train_accs, color=colors['train'], 
                       linewidth=3, label='Training Accuracy', alpha=0.8)
        axes[0, 1].plot(epochs_smooth, self.val_accs, color=colors['val'], 
                       linewidth=3, label='Validation Accuracy', alpha=0.8)
        axes[0, 1].fill_between(epochs_smooth, self.train_accs, alpha=0.2, color=colors['train'])
        axes[0, 1].fill_between(epochs_smooth, self.val_accs, alpha=0.2, color=colors['val'])
        axes[0, 1].set_xlabel('Epoch', fontsize=14)
        axes[0, 1].set_ylabel('Accuracy (%)', fontsize=14)
        axes[0, 1].set_title('Training vs Validation Accuracy', fontsize=16, fontweight='bold')
        axes[0, 1].legend(fontsize=12)
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Overfitting indicators
        loss_gaps = [val - train for val, train in zip(self.val_losses, self.train_losses)]
        acc_gaps = [train - val for train, val in zip(self.train_accs, self.val_accs)]
        
        ax3_twin = axes[1, 0].twinx()
        axes[1, 0].plot(epochs_smooth, loss_gaps, color=colors['gap'], linewidth=3, label='Loss Gap')
        axes[1, 0].axhline(y=self.overfitting_threshold, color=colors['threshold'], 
                          linestyle='--', linewidth=2, label=f'Threshold ({self.overfitting_threshold})')
        ax3_twin.plot(epochs_smooth, acc_gaps, 'purple', linewidth=2, alpha=0.7, label='Acc Gap (%)')
        
        axes[1, 0].set_xlabel('Epoch', fontsize=14)
        axes[1, 0].set_ylabel('Loss Gap (Val - Train)', fontsize=14, color=colors['gap'])
        ax3_twin.set_ylabel('Accuracy Gap (Train - Val) %', fontsize=14, color='purple')
        axes[1, 0].set_title('Overfitting Detection Metrics', fontsize=16, fontweight='bold')
        axes[1, 0].legend(loc='upper left', fontsize=12)
        ax3_twin.legend(loc='upper right', fontsize=12)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 4: Training statistics
        if hasattr(self, 'learning_rates') and len(self.learning_rates) > 0:
            ax4_twin = axes[1, 1].twinx()
            axes[1, 1].semilogy(epochs_smooth, self.learning_rates, 'purple', linewidth=3, 
                               label='Learning Rate')
            if hasattr(self, 'dropout_rates'):
                ax4_twin.plot(epochs_smooth, self.dropout_rates, 'green', linewidth=3, 
                             label='Dropout Rate')
                ax4_twin.set_ylabel('Dropout Rate', fontsize=14, color='green')
            axes[1, 1].set_xlabel('Epoch', fontsize=14)
            axes[1, 1].set_ylabel('Learning Rate (log scale)', fontsize=14, color='purple')
            axes[1, 1].set_title('Learning Rate & Regularization Schedule', fontsize=16, fontweight='bold')
            axes[1, 1].legend(loc='upper left', fontsize=12)
            if hasattr(self, 'dropout_rates'):
                ax4_twin.legend(loc='upper right', fontsize=12)
        else:
            # Training summary statistics
            best_val_acc = max(self.val_accs)
            best_val_epoch = self.val_accs.index(best_val_acc) + 1
            final_gap = self.train_accs[-1] - self.val_accs[-1]
            
            axes[1, 1].text(0.1, 0.8, f'Best Val Accuracy: {best_val_acc:.2f}%', 
                           transform=axes[1, 1].transAxes, fontsize=14, fontweight='bold')
            axes[1, 1].text(0.1, 0.7, f'Best Epoch: {best_val_epoch}', 
                           transform=axes[1, 1].transAxes, fontsize=14)
            axes[1, 1].text(0.1, 0.6, f'Final Acc Gap: {final_gap:.2f}%', 
                           transform=axes[1, 1].transAxes, fontsize=14)
            axes[1, 1].text(0.1, 0.5, f'Overfitting Alerts: {len(self.overfitting_alerts)}', 
                           transform=axes[1, 1].transAxes, fontsize=14)
            
            risk_color = 'red' if len(self.overfitting_alerts) >= 3 else 'orange' if len(self.overfitting_alerts) >= 1 else 'green'
            risk_level = 'HIGH' if len(self.overfitting_alerts) >= 3 else 'MEDIUM' if len(self.overfitting_alerts) >= 1 else 'LOW'
            axes[1, 1].text(0.1, 0.3, f'Overfitting Risk: {risk_level}', 
                           transform=axes[1, 1].transAxes, fontsize=14, 
                           fontweight='bold', color=risk_color)
            axes[1, 1].set_title('Training Summary Statistics', fontsize=16, fontweight='bold')
            axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        # Save final curves
        final_path = os.path.join(self.save_dir, 'final_training_validation_curves.png')
        plt.savefig(final_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"ðŸ“ˆ Saved comprehensive training curves to {final_path}")
        return final_path

# Model size configurations (OpenCLIP SigLIP models)
MODEL_CONFIGS = {
    'tiny': {
        'model': 'ViT-B-16-SigLIP-256',
        'resolution': 256,
        'params': '86M',
        'description': 'Small SigLIP model, fast'
    },
    'small': {
        'model': 'ViT-B-16-SigLIP-384', 
        'resolution': 384,
        'params': '86M',
        'description': 'Small SigLIP model, higher resolution'
    },
    'medium': {
        'model': 'ViT-L-16-SigLIP-384', 
        'resolution': 384,
        'params': '307M',
        'description': 'Large SigLIP model - good balance (default)'
    },
    'large': {
        'model': 'ViT-SO400M-16-SigLIP2-512',
        'resolution': 512, 
        'params': '400M',
        'description': 'Best SigLIP-2 model for maximum accuracy'
    }
}

class LightweightAttention(nn.Module):
    """Lightweight attention mechanism for smaller models"""
    def __init__(self, dim, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)
        
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        return x

class FastBinaryClassifier(nn.Module):
    """Fast, lightweight binary classifier for fake image detection"""
    def __init__(self, model_size='small', device='cuda', use_openclip=False, 
                 dropout_rate=0.1, use_lightweight_attention=True):
        super().__init__()
        
        self.model_size = model_size
        self.config = MODEL_CONFIGS[model_size]
        self.resolution = self.config['resolution']
        self.use_openclip = use_openclip
        
        # This should never execute now
        if False:
            # Use TIMM models (default - faster and more efficient)
            self.backbone = timm.create_model(
                'dummy', 
                pretrained=True,
                num_classes=0,  # Remove classifier head
                global_pool=''
            )
            pass  # Old TIMM code removed
            pass  # This should never execute now
        
        if not OPENCLIP_AVAILABLE:
            raise ImportError("open_clip required. Install with: pip install open-clip-torch")
        
        # Use OpenCLIP SigLIP models
        pretrained_name = 'webli' if 'SigLIP2' in self.config['model'] else 'webli'
        self.backbone, _, _ = open_clip.create_model_and_transforms(
            self.config['model'], 
            pretrained=pretrained_name,
            device=device
        )
        
        # Get feature dimension
        if hasattr(self.backbone, 'embed_dim'):
            self.feature_dim = self.backbone.embed_dim
        else:
            with torch.no_grad():
                dummy_input = torch.randn(1, 3, self.resolution, self.resolution, device=device)
                dummy_features = self.backbone.encode_image(dummy_input)
                self.feature_dim = dummy_features.shape[-1]
        self.use_clip = True
        print(f"âœ… Using SigLIP model: {self.config['model']}")
        
        # Attention mechanism based on model size
        if use_lightweight_attention and model_size in ['tiny', 'small']:
            self.attention = LightweightAttention(self.feature_dim, num_heads=4)
            self.use_attention = True
        elif model_size == 'large':
            # Enhanced attention for large model
            num_heads = min(8, self.feature_dim // 64)
            self.attention = nn.MultiheadAttention(
                self.feature_dim, num_heads, dropout=dropout_rate, batch_first=True
            )
            self.use_attention = True
        else:
            self.use_attention = False
        
        # Classifier head scaled by model size
        if model_size == 'tiny':
            # Ultra-minimal head for tiny model
            self.classifier = nn.Sequential(
                nn.Dropout(dropout_rate * 0.5),
                nn.Linear(self.feature_dim, 1)
            )
        elif model_size == 'small':
            # Single hidden layer
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_dim, self.feature_dim // 4),
                nn.GELU(),
                nn.Dropout(dropout_rate),
                nn.Linear(self.feature_dim // 4, 1)
            )
        else:  # medium, large
            # Standard head for medium/large
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_dim, self.feature_dim // 2),
                nn.GELU(), 
                nn.Dropout(dropout_rate),
                nn.Linear(self.feature_dim // 2, self.feature_dim // 4),
                nn.GELU(),
                nn.Dropout(dropout_rate * 0.5),
                nn.Linear(self.feature_dim // 4, 1)
            )
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(self.feature_dim)
        
        # Initialize weights
        self.apply(self._init_weights)
        
        print(f"ðŸš€ Fast Model: {model_size.upper()} ({self.config['params']})")
        print(f"ðŸ“ Resolution: {self.resolution}px, Features: {self.feature_dim}")
        print(f"ðŸ’¡ Description: {self.config['description']}")
        if self.use_attention:
            print(f"ðŸŽ¯ Lightweight attention: Enabled")
        print(f"âš¡ Backend: {'OpenCLIP' if self.use_clip else 'TIMM (optimized)'}")
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
                
    def get_regularization_loss(self, l1_lambda=1e-5, l2_lambda=1e-4):
        """Calculate L1 and L2 regularization losses"""
        l1_loss = 0.0
        l2_loss = 0.0
        
        for param in self.classifier.parameters():
            if param.requires_grad:
                l1_loss += torch.sum(torch.abs(param))
                l2_loss += torch.sum(param ** 2)
        
        return l1_lambda * l1_loss + l2_lambda * l2_loss
    
    def forward(self, x, return_features=False):
        # Resize input if needed
        if x.shape[-1] != self.resolution:
            x = F.interpolate(x, size=(self.resolution, self.resolution), mode='bilinear', align_corners=False)
        
        # Extract features
        if self.use_clip:
            features = self.backbone.encode_image(x)
        else:
            features = self.backbone(x)
            if len(features.shape) == 4:  # If spatial features, global average pool
                features = F.adaptive_avg_pool2d(features, 1).flatten(1)
        
        # Normalize features
        features = features / features.norm(dim=-1, keepdim=True)
        features = self.layer_norm(features)
        
        # Apply attention if enabled
        if self.use_attention:
            if features.dim() == 2:
                features = features.unsqueeze(1)  # Add sequence dimension
            
            if isinstance(self.attention, LightweightAttention):
                features = self.attention(features)
            else:
                # Full multi-head attention
                features, _ = self.attention(features, features, features)
            
            features = features.squeeze(1)  # Remove sequence dimension
        
        if return_features:
            return features
        
        # Classification
        logits = self.classifier(features)
        return logits.squeeze(-1)

# Backward compatibility
EnhancedBinaryClassifier = FastBinaryClassifier
BinaryClassifier = FastBinaryClassifier

def test_time_augmentation(model, images, device, gpu_transform=None, n_tta=5):
    """Apply test-time augmentation for improved accuracy"""
    model.eval()
    
    # Base prediction
    with torch.no_grad():
        base_logits = model(images)
    
    predictions = [base_logits]
    
    # Apply different augmentations
    for _ in range(n_tta):
        # Random horizontal flip
        aug_images = images.clone()
        if torch.rand(1) > 0.5:
            aug_images = torch.flip(aug_images, dims=[3])
        
        # Small random crops
        if torch.rand(1) > 0.5:
            h, w = aug_images.shape[-2:]
            crop_size = int(0.95 * min(h, w))
            top = torch.randint(0, h - crop_size + 1, (1,)).item()
            left = torch.randint(0, w - crop_size + 1, (1,)).item()
            aug_images = aug_images[:, :, top:top+crop_size, left:left+crop_size]
            aug_images = F.interpolate(aug_images, size=(h, w), mode='bilinear', align_corners=False)
        
        with torch.no_grad():
            aug_logits = model(aug_images)
            predictions.append(aug_logits)
    
    # Average predictions
    return torch.stack(predictions).mean(dim=0)

def label_smoothing_loss(pred, target, smoothing=0.1):
    """Apply label smoothing for better generalization"""
    target = target.float()
    target = target * (1 - smoothing) + 0.5 * smoothing
    return F.binary_cross_entropy_with_logits(pred, target)

def train_epoch(model, dataloader, criterion, optimizer, device, scaler=None, accumulate_grad_batches=1, gpu_transform=None, track_gradients=False, ema=None, use_label_smoothing=False, mixup_alpha=0.2):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    gradient_norms = []
    
    pbar = tqdm(dataloader, desc="Training")
    for batch_idx, (images, labels) in enumerate(pbar):
        try:
            images = images.to(device, non_blocking=True, memory_format=torch.channels_last)
            labels = labels.to(device, non_blocking=True)
            
            # Apply GPU transforms if available
            if gpu_transform is not None:
                images = gpu_transform(images)
            
            # Apply MixUp augmentation
            if mixup_alpha > 0 and torch.rand(1) > 0.5:
                lam = np.random.beta(mixup_alpha, mixup_alpha)
                batch_size = images.size(0)
                index = torch.randperm(batch_size, device=device)
                mixed_images = lam * images + (1 - lam) * images[index]
                mixed_labels = lam * labels.float() + (1 - lam) * labels[index].float()
            else:
                mixed_images = images
                mixed_labels = labels.float()
            
            # Mixed precision forward pass
            if scaler is not None:
                with torch.amp.autocast('cuda'):
                    logits = model(mixed_images)
                    if use_label_smoothing:
                        loss = label_smoothing_loss(logits, mixed_labels, smoothing=0.1) / accumulate_grad_batches
                    else:
                        loss = criterion(logits, mixed_labels) / accumulate_grad_batches
                
                scaler.scale(loss).backward()
                
                if (batch_idx + 1) % accumulate_grad_batches == 0:
                    scaler.unscale_(optimizer)
                    if track_gradients:
                        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        gradient_norms.append(grad_norm.item())
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad(set_to_none=True)
                    if ema is not None:
                        ema.update()
            else:
                logits = model(mixed_images)
                if use_label_smoothing:
                    loss = label_smoothing_loss(logits, mixed_labels, smoothing=0.1) / accumulate_grad_batches
                else:
                    loss = criterion(logits, mixed_labels) / accumulate_grad_batches
                loss.backward()
                
                if (batch_idx + 1) % accumulate_grad_batches == 0:
                    if track_gradients:
                        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        gradient_norms.append(grad_norm.item())
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad(set_to_none=True)
                    if ema is not None:
                        ema.update()
            
            # Statistics
            total_loss += loss.item() * accumulate_grad_batches
            predicted = (torch.sigmoid(logits) > 0.5).long()
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            
            pbar.set_postfix({
                'Loss': f'{loss.item() * accumulate_grad_batches:.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        except RuntimeError as e:
            if "CUDA out of memory" in str(e) or "INTERNAL ASSERT FAILED" in str(e):
                print(f"\nðŸš¨ CUDA Memory Error at batch {batch_idx}: {e}")
                print("ðŸ’¡ Try reducing batch size or enabling gradient checkpointing")
                
                # Clear cache and try to continue
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    import gc
                    gc.collect()
                
                # Skip this batch and continue
                continue
            else:
                raise e
    
    return total_loss / len(dataloader), 100. * correct / total, gradient_norms

def evaluate(model, dataloader, criterion, device, gpu_transform=None, use_tta=False, ema=None):
    # Apply EMA weights if available
    if ema is not None:
        ema.apply_shadow()
    model.eval()
    total_loss = 0
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Evaluating"):
            images = images.to(device, non_blocking=True, memory_format=torch.channels_last)
            labels = labels.to(device, non_blocking=True)
            
            # Apply GPU transforms if available
            if gpu_transform is not None:
                images = gpu_transform(images)
            
            # Use test-time augmentation if enabled
            if use_tta:
                logits = test_time_augmentation(model, images, device, gpu_transform, n_tta=5)
                if device.type == 'cuda':
                    with torch.amp.autocast('cuda'):
                        loss = criterion(logits, labels.float())
                else:
                    loss = criterion(logits, labels.float())
            else:
                # Use mixed precision for evaluation too
                if device.type == 'cuda':
                    with torch.amp.autocast('cuda'):
                        logits = model(images)
                        loss = criterion(logits, labels.float())
                else:
                    logits = model(images)
                    loss = criterion(logits, labels.float())
            
            total_loss += loss.item()
            probs = torch.sigmoid(logits)
            
            all_labels.append(labels.cpu())
            all_probs.append(probs.cpu())
    
    # Calculate metrics
    all_labels = torch.cat(all_labels).numpy()
    all_probs = torch.cat(all_probs).numpy()
    all_preds = (all_probs > 0.5).astype(int)
    
    accuracy = accuracy_score(all_labels, all_preds)
    balanced_acc = balanced_accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)
    auc = roc_auc_score(all_labels, all_probs)
    mcc = matthews_corrcoef(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)
    
    avg_loss = total_loss / len(dataloader)
    
    # Restore original weights if EMA was used
    if ema is not None:
        ema.restore()
    
    return avg_loss, accuracy, balanced_acc, precision, recall, f1, auc, mcc, cm, all_labels, all_probs

def save_plots(labels, probs, save_dir, epoch=None, prefix=""):
    """Save comprehensive publication-quality plots"""
    preds = (probs > 0.5).astype(int)
    cm = confusion_matrix(labels, preds)
    
    # Set publication style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 12,
        'axes.labelsize': 14,
        'axes.titlesize': 16,
        'xtick.labelsize': 12,
        'ytick.labelsize': 12,
        'legend.fontsize': 12,
        'figure.titlesize': 18
    })
    
    # Create comprehensive figure
    fig = plt.figure(figsize=(16, 12))
    
    # 1. Confusion Matrix (normalized)
    ax1 = plt.subplot(2, 3, 1)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues', ax=ax1,
                cbar_kws={'label': 'Normalized Count'})
    ax1.set_title('Normalized Confusion Matrix')
    ax1.set_xlabel('Predicted Label')
    ax1.set_ylabel('True Label')
    ax1.set_xticklabels(['Real', 'Fake'])
    ax1.set_yticklabels(['Real', 'Fake'])
    
    # 2. Raw Confusion Matrix
    ax2 = plt.subplot(2, 3, 2)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', ax=ax2,
                cbar_kws={'label': 'Count'})
    ax2.set_title('Raw Confusion Matrix')
    ax2.set_xlabel('Predicted Label')
    ax2.set_ylabel('True Label')
    ax2.set_xticklabels(['Real', 'Fake'])
    ax2.set_yticklabels(['Real', 'Fake'])
    
    # 3. ROC Curve
    ax3 = plt.subplot(2, 3, 3)
    fpr, tpr, _ = roc_curve(labels, probs)
    auc = roc_auc_score(labels, probs)
    ax3.plot(fpr, tpr, linewidth=3, label=f'ROC Curve (AUC = {auc:.3f})')
    ax3.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
    ax3.set_xlabel('False Positive Rate')
    ax3.set_ylabel('True Positive Rate')
    ax3.set_title('ROC Curve')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_xlim([-0.02, 1.02])
    ax3.set_ylim([-0.02, 1.02])
    
    # 4. Precision-Recall Curve
    ax4 = plt.subplot(2, 3, 4)
    precision, recall, _ = precision_recall_curve(labels, probs)
    ap = average_precision_score(labels, probs)
    ax4.plot(recall, precision, linewidth=3, label=f'PR Curve (AP = {ap:.3f})')
    ax4.axhline(y=labels.mean(), color='k', linestyle='--', linewidth=2, 
                label=f'Random (AP = {labels.mean():.3f})')
    ax4.set_xlabel('Recall')
    ax4.set_ylabel('Precision')
    ax4.set_title('Precision-Recall Curve')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    ax4.set_xlim([-0.02, 1.02])
    ax4.set_ylim([-0.02, 1.02])
    
    # 5. Score Distribution
    ax5 = plt.subplot(2, 3, 5)
    real_scores = probs[labels == 0]
    fake_scores = probs[labels == 1]
    ax5.hist(real_scores, bins=50, alpha=0.7, label=f'Real (n={len(real_scores)})', 
             color='blue', density=True)
    ax5.hist(fake_scores, bins=50, alpha=0.7, label=f'Fake (n={len(fake_scores)})', 
             color='red', density=True)
    ax5.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')
    ax5.set_xlabel('Classification Score')
    ax5.set_ylabel('Density')
    ax5.set_title('Score Distribution by Class')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. Performance Metrics Bar Chart
    ax6 = plt.subplot(2, 3, 6)
    metrics = ['Accuracy', 'Balanced Acc', 'Precision', 'Recall', 'F1', 'AUC']
    values = [accuracy_score(labels, preds), balanced_accuracy_score(labels, preds),
              precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)[0],
              precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)[1],
              precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)[2],
              auc]
    
    bars = ax6.bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])
    ax6.set_ylabel('Score')
    ax6.set_title('Performance Metrics')
    ax6.set_ylim([0, 1])
    ax6.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, value in zip(bars, values):
        ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Save plot
    if epoch is not None:
        plot_path = os.path.join(save_dir, f'{prefix}publication_metrics_epoch_{epoch+1}.png')
    else:
        plot_path = os.path.join(save_dir, f'{prefix}publication_metrics.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"Saved publication plots to {plot_path}")

def save_training_curves(train_losses, train_accs, val_losses, val_accs, val_f1s, save_dir, gradient_norms=None):
    """Save comprehensive training curves for publication"""
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 12,
        'axes.labelsize': 14,
        'axes.titlesize': 16,
        'xtick.labelsize': 12,
        'ytick.labelsize': 12,
        'legend.fontsize': 12,
        'figure.titlesize': 18
    })
    
    epochs = range(1, len(train_losses) + 1)
    
    # Create a comprehensive training plot
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Loss Curves
    ax1.plot(epochs, train_losses, 'b-', linewidth=3, label='Training Loss', marker='o', markersize=4)
    ax1.plot(epochs, val_losses, 'r-', linewidth=3, label='Validation Loss', marker='s', markersize=4)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss per Epoch')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim([1, len(epochs)])
    
    # 2. Accuracy Curves (Training vs Validation)
    ax2.plot(epochs, [acc/100 for acc in train_accs], 'b-', linewidth=3, label='Training Accuracy', marker='o', markersize=4)
    ax2.plot(epochs, [acc/100 for acc in val_accs], 'r-', linewidth=3, label='Validation Accuracy', marker='s', markersize=4)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Training and Validation Accuracy per Epoch')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0, 1])
    ax2.set_xlim([1, len(epochs)])
    
    # 3. Validation Metrics Combined
    ax3.plot(epochs, [acc/100 for acc in val_accs], 'r-', linewidth=3, label='Validation Accuracy', marker='s', markersize=4)
    ax3.plot(epochs, val_f1s, 'g-', linewidth=3, label='Validation F1 Score', marker='^', markersize=4)
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Score')
    ax3.set_title('Validation Accuracy and F1 Score per Epoch')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim([0, 1])
    ax3.set_xlim([1, len(epochs)])
    
    # 4. Overfitting Detection (Gap Analysis)
    loss_gaps = [val - train for val, train in zip(val_losses, train_losses)]
    acc_gaps = [train - val for train, val in zip(train_accs, val_accs)]
    
    ax4_twin = ax4.twinx()
    ax4.plot(epochs, loss_gaps, 'orange', linewidth=3, label='Loss Gap (Val-Train)', marker='d', markersize=4)
    ax4.axhline(y=0.05, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Overfitting Threshold')
    ax4_twin.plot(epochs, acc_gaps, 'purple', linewidth=2, alpha=0.8, label='Accuracy Gap (Train-Val) %', marker='x', markersize=3)
    
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Loss Gap (Val - Train)', color='orange')
    ax4_twin.set_ylabel('Accuracy Gap (Train - Val) %', color='purple')
    ax4.set_title('Overfitting Detection Metrics')
    ax4.legend(loc='upper left')
    ax4_twin.legend(loc='upper right')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save training curves
    curves_path = os.path.join(save_dir, 'training_curves.png')
    plt.savefig(curves_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"Saved training curves to {curves_path}")
    
    # Save individual metric plots for publication
    save_individual_metric_plots(train_losses, train_accs, val_losses, val_accs, val_f1s, save_dir)

def save_individual_metric_plots(train_losses, train_accs, val_losses, val_accs, val_f1s, save_dir):
    """Save individual metric plots for publication figures"""
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 14,
        'axes.labelsize': 16,
        'axes.titlesize': 18,
        'xtick.labelsize': 14,
        'ytick.labelsize': 14,
        'legend.fontsize': 14
    })
    
    epochs = range(1, len(train_losses) + 1)
    
    # Individual Loss Plot
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    ax.plot(epochs, train_losses, 'b-', linewidth=3, label='Training Loss', marker='o', markersize=6)
    ax.plot(epochs, val_losses, 'r-', linewidth=3, label='Validation Loss', marker='s', markersize=6)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('Training and Validation Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xlim([1, len(epochs)])
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'loss_curves.png'), dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Individual Accuracy Plot
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    ax.plot(epochs, [acc/100 for acc in train_accs], 'b-', linewidth=3, label='Training Accuracy', marker='o', markersize=6)
    ax.plot(epochs, [acc/100 for acc in val_accs], 'r-', linewidth=3, label='Validation Accuracy', marker='s', markersize=6)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Accuracy')
    ax.set_title('Training and Validation Accuracy')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 1])
    ax.set_xlim([1, len(epochs)])
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'accuracy_curves.png'), dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Individual F1 Plot
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    ax.plot(epochs, val_f1s, 'g-', linewidth=3, label='Validation F1 Score', marker='^', markersize=6)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('F1 Score')
    ax.set_title('Validation F1 Score Progression')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 1])
    ax.set_xlim([1, len(epochs)])
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'f1_curve.png'), dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"Saved individual metric plots: loss_curves.png, accuracy_curves.png, f1_curve.png")

def bootstrap_confidence_intervals(labels, probs, n_bootstrap=1000, confidence_level=0.95):
    """Calculate bootstrap confidence intervals for metrics"""
    np.random.seed(42)  # For reproducibility
    
    metrics = []
    for _ in range(n_bootstrap):
        # Bootstrap sample
        indices = resample(range(len(labels)), random_state=np.random.randint(0, 10000))
        y_boot = labels[indices]
        p_boot = probs[indices]
        pred_boot = (p_boot > 0.5).astype(int)
        
        # Calculate metrics
        if len(np.unique(y_boot)) > 1 and len(np.unique(pred_boot)) > 1:
            acc = accuracy_score(y_boot, pred_boot)
            prec, rec, f1, _ = precision_recall_fscore_support(y_boot, pred_boot, average='binary', zero_division=0)
            auc = roc_auc_score(y_boot, p_boot)
            metrics.append([acc, prec, rec, f1, auc])
        else:
            metrics.append([0, 0, 0, 0, 0.5])
    
    metrics = np.array(metrics)
    alpha = 1 - confidence_level
    lower_percentile = 100 * (alpha / 2)
    upper_percentile = 100 * (1 - alpha / 2)
    
    ci_lower = np.percentile(metrics, lower_percentile, axis=0)
    ci_upper = np.percentile(metrics, upper_percentile, axis=0)
    
    return ci_lower, ci_upper

def statistical_significance_tests(labels, probs, save_dir):
    """Perform statistical significance tests and save results"""
    preds = (probs > 0.5).astype(int)
    cm = confusion_matrix(labels, preds)
    
    results = {}
    
    # Chi-square test for independence
    try:
        chi2, p_chi2, dof, expected = chi2_contingency(cm)
        results['chi_square'] = {'statistic': chi2, 'p_value': p_chi2, 'dof': dof}
    except:
        results['chi_square'] = {'statistic': np.nan, 'p_value': np.nan, 'dof': np.nan}
    
    # Fisher's exact test (for 2x2 contingency table)
    try:
        oddsratio, p_fisher = fisher_exact(cm)
        results['fisher_exact'] = {'odds_ratio': oddsratio, 'p_value': p_fisher}
    except:
        results['fisher_exact'] = {'odds_ratio': np.nan, 'p_value': np.nan}
    
    # McNemar's test (comparing against random classifier)
    random_preds = np.random.binomial(1, 0.5, len(labels))
    try:
        # Create contingency table for McNemar's test
        correct_model = (preds == labels)
        correct_random = (random_preds == labels)
        
        both_correct = np.sum(correct_model & correct_random)
        model_correct_random_wrong = np.sum(correct_model & ~correct_random)
        model_wrong_random_correct = np.sum(~correct_model & correct_random)
        both_wrong = np.sum(~correct_model & ~correct_random)
        
        # McNemar statistic
        b = model_wrong_random_correct
        c = model_correct_random_wrong
        if b + c > 0:
            mcnemar_stat = (abs(b - c) - 1) ** 2 / (b + c)
            p_mcnemar = 1 - stats.chi2.cdf(mcnemar_stat, 1)
        else:
            mcnemar_stat = 0
            p_mcnemar = 1.0
            
        results['mcnemar'] = {'statistic': mcnemar_stat, 'p_value': p_mcnemar}
    except:
        results['mcnemar'] = {'statistic': np.nan, 'p_value': np.nan}
    
    # Bootstrap confidence intervals
    ci_lower, ci_upper = bootstrap_confidence_intervals(labels, probs)
    results['confidence_intervals'] = {
        'accuracy': {'lower': ci_lower[0], 'upper': ci_upper[0]},
        'precision': {'lower': ci_lower[1], 'upper': ci_upper[1]},
        'recall': {'lower': ci_lower[2], 'upper': ci_upper[2]},
        'f1': {'lower': ci_lower[3], 'upper': ci_upper[3]},
        'auc': {'lower': ci_lower[4], 'upper': ci_upper[4]}
    }
    
    # Save results to JSON
    with open(os.path.join(save_dir, 'statistical_tests.json'), 'w') as f:
        # Convert numpy types to Python types for JSON serialization
        def convert_types(obj):
            if isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, dict):
                return {key: convert_types(value) for key, value in obj.items()}
            return obj
        
        json.dump(convert_types(results), f, indent=2)
    
    return results

def save_publication_table(labels, probs, save_dir, prefix=""):
    """Generate publication-ready performance table"""
    preds = (probs > 0.5).astype(int)
    
    # Calculate all metrics
    accuracy = accuracy_score(labels, preds)
    balanced_acc = balanced_accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)
    auc = roc_auc_score(labels, probs)
    ap = average_precision_score(labels, probs)
    mcc = matthews_corrcoef(labels, preds)
    kappa = cohen_kappa_score(labels, preds)
    
    # Confusion matrix metrics
    cm = confusion_matrix(labels, preds)
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative predictive value
    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive predictive value (same as precision)
    
    # Bootstrap confidence intervals
    ci_lower, ci_upper = bootstrap_confidence_intervals(labels, probs)
    
    # Create DataFrame
    metrics_data = {
        'Metric': ['Accuracy', 'Balanced Accuracy', 'Precision (PPV)', 'Recall (Sensitivity)', 
                  'Specificity', 'NPV', 'F1-Score', 'AUC-ROC', 'AP', 'MCC', 'Cohen\'s Îº'],
        'Value': [accuracy, balanced_acc, precision, sensitivity, specificity, npv, f1, auc, ap, mcc, kappa],
        '95% CI Lower': [ci_lower[0], np.nan, ci_lower[1], ci_lower[2], np.nan, np.nan, ci_lower[3], ci_lower[4], np.nan, np.nan, np.nan],
        '95% CI Upper': [ci_upper[0], np.nan, ci_upper[1], ci_upper[2], np.nan, np.nan, ci_upper[3], ci_upper[4], np.nan, np.nan, np.nan]
    }
    
    df = pd.DataFrame(metrics_data)
    
    # Format for publication
    df['Formatted'] = df.apply(lambda row: 
        f"{row['Value']:.3f} ({row['95% CI Lower']:.3f}-{row['95% CI Upper']:.3f})" 
        if not pd.isna(row['95% CI Lower']) 
        else f"{row['Value']:.3f}", axis=1)
    
    # Save as CSV
    df.to_csv(os.path.join(save_dir, f'{prefix}performance_table.csv'), index=False)
    
    # Save as LaTeX table
    latex_table = df[['Metric', 'Formatted']].to_latex(
        index=False, 
        column_format='lc',
        caption='CIFAKE Performance Metrics with 95\\% Bootstrap Confidence Intervals',
        label='tab:cifake_performance',
        escape=False
    )
    
    with open(os.path.join(save_dir, f'{prefix}performance_table.tex'), 'w') as f:
        f.write(latex_table)
    
    return df

def save_error_analysis_plots(labels, probs, save_dir, prefix=""):
    """Analyze and visualize prediction errors"""
    preds = (probs > 0.5).astype(int)
    
    # Identify different types of predictions
    correct = (preds == labels)
    incorrect = ~correct
    
    # True/False Positives/Negatives
    tp_mask = (labels == 1) & (preds == 1)
    fp_mask = (labels == 0) & (preds == 1)
    tn_mask = (labels == 0) & (preds == 0)
    fn_mask = (labels == 1) & (preds == 0)
    
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 12,
        'axes.labelsize': 14,
        'axes.titlesize': 16,
        'legend.fontsize': 12
    })
    
    # 1. Error Analysis by Confidence
    plt.figure(figsize=(12, 8))
    
    # Subplot 1: Confidence distribution by prediction type
    plt.subplot(2, 2, 1)
    tp_probs = probs[tp_mask]
    fp_probs = probs[fp_mask]
    tn_probs = 1 - probs[tn_mask]  # Convert to confidence in "real" class
    fn_probs = 1 - probs[fn_mask]
    
    plt.hist([tp_probs, fp_probs, tn_probs, fn_probs], bins=20, alpha=0.7, 
             label=['True Pos', 'False Pos', 'True Neg', 'False Neg'],
             color=['green', 'red', 'blue', 'orange'])
    plt.xlabel('Model Confidence')
    plt.ylabel('Count')
    plt.title('Confidence Distribution by Prediction Type')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Confidence vs Accuracy
    confidence_bins = np.linspace(0, 1, 11)
    bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
    bin_accuracies = []
    bin_counts = []
    
    for i in range(len(confidence_bins) - 1):
        mask = (probs >= confidence_bins[i]) & (probs < confidence_bins[i + 1])
        if np.sum(mask) > 0:
            bin_acc = np.mean(correct[mask])
            bin_accuracies.append(bin_acc)
            bin_counts.append(np.sum(mask))
        else:
            bin_accuracies.append(0)
            bin_counts.append(0)
    
    plt.subplot(2, 2, 2)
    plt.plot(bin_centers, bin_accuracies, 'o-', linewidth=2, markersize=8, color='purple')
    plt.xlabel('Confidence Bin Center')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Model Confidence')
    plt.grid(True, alpha=0.3)
    plt.ylim([0, 1])
    
    # Subplot 3: Sample counts per bin
    plt.subplot(2, 2, 3)
    plt.bar(bin_centers, bin_counts, width=0.08, alpha=0.7, color='cyan')
    plt.xlabel('Confidence Bin Center')
    plt.ylabel('Number of Samples')
    plt.title('Sample Distribution Across Confidence Bins')
    plt.grid(True, alpha=0.3)
    
    # Subplot 4: Error rates by confidence quartiles
    plt.subplot(2, 2, 4)
    quartiles = np.percentile(probs, [25, 50, 75])
    q_labels = ['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)']
    q_errors = []
    
    for i in range(4):
        if i == 0:
            mask = probs <= quartiles[0]
        elif i == 1:
            mask = (probs > quartiles[0]) & (probs <= quartiles[1])
        elif i == 2:
            mask = (probs > quartiles[1]) & (probs <= quartiles[2])
        else:
            mask = probs > quartiles[2]
        
        if np.sum(mask) > 0:
            error_rate = 1 - np.mean(correct[mask])
            q_errors.append(error_rate)
        else:
            q_errors.append(0)
    
    plt.bar(q_labels, q_errors, color=['red', 'orange', 'yellow', 'green'], alpha=0.7)
    plt.xlabel('Confidence Quartile')
    plt.ylabel('Error Rate')
    plt.title('Error Rate by Confidence Quartile')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'{prefix}error_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    return {
        'confidence_accuracy_correlation': np.corrcoef(probs, correct.astype(int))[0, 1],
        'high_confidence_accuracy': np.mean(correct[probs > 0.9]) if np.sum(probs > 0.9) > 0 else np.nan,
        'low_confidence_accuracy': np.mean(correct[probs < 0.1]) if np.sum(probs < 0.1) > 0 else np.nan
    }

def save_uncertainty_analysis(model, dataloader, device, save_dir, gpu_transform=None, n_passes=10):
    """Analyze model uncertainty using Monte Carlo Dropout"""
    model.train()  # Enable dropout for uncertainty estimation
    
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Uncertainty Analysis", leave=False):
            images = images.to(device, non_blocking=True, memory_format=torch.channels_last)
            labels = labels.to(device, non_blocking=True)
            
            # Apply GPU transforms if available
            if gpu_transform is not None:
                images = gpu_transform(images)
            
            # Multiple forward passes for uncertainty estimation
            predictions = []
            for _ in range(n_passes):
                if device.type == 'cuda':
                    with torch.amp.autocast('cuda'):
                        logits = model(images)
                        probs = torch.sigmoid(logits)
                else:
                    logits = model(images)
                    probs = torch.sigmoid(logits)
                
                predictions.append(probs.cpu().numpy())
            
            predictions = np.array(predictions)  # Shape: (n_passes, batch_size)
            all_predictions.append(predictions)
            all_labels.append(labels.cpu().numpy())
    
    # Combine all predictions
    all_predictions = np.concatenate(all_predictions, axis=1)  # Shape: (n_passes, total_samples)
    all_labels = np.concatenate(all_labels)
    
    # Calculate uncertainty metrics
    mean_predictions = np.mean(all_predictions, axis=0)
    prediction_std = np.std(all_predictions, axis=0)
    
    # Epistemic uncertainty (model uncertainty)
    epistemic_uncertainty = prediction_std
    
    # Aleatoric uncertainty (data uncertainty) - approximated
    aleatoric_uncertainty = mean_predictions * (1 - mean_predictions)
    
    # Total uncertainty
    total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty
    
    # Plot uncertainty analysis
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({'font.size': 12, 'axes.labelsize': 14, 'axes.titlesize': 16})
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. Epistemic uncertainty distribution
    axes[0, 0].hist(epistemic_uncertainty, bins=30, alpha=0.7, color='blue', edgecolor='black')
    axes[0, 0].set_xlabel('Epistemic Uncertainty')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Distribution of Epistemic Uncertainty')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Aleatoric uncertainty distribution
    axes[0, 1].hist(aleatoric_uncertainty, bins=30, alpha=0.7, color='red', edgecolor='black')
    axes[0, 1].set_xlabel('Aleatoric Uncertainty')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title('Distribution of Aleatoric Uncertainty')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Total uncertainty distribution
    axes[0, 2].hist(total_uncertainty, bins=30, alpha=0.7, color='purple', edgecolor='black')
    axes[0, 2].set_xlabel('Total Uncertainty')
    axes[0, 2].set_ylabel('Frequency')
    axes[0, 2].set_title('Distribution of Total Uncertainty')
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. Uncertainty vs Accuracy
    correct = (mean_predictions > 0.5) == all_labels
    
    # Bin by uncertainty and calculate accuracy
    uncertainty_bins = np.linspace(0, total_uncertainty.max(), 10)
    bin_centers = (uncertainty_bins[:-1] + uncertainty_bins[1:]) / 2
    bin_accuracies = []
    
    for i in range(len(uncertainty_bins) - 1):
        mask = (total_uncertainty >= uncertainty_bins[i]) & (total_uncertainty < uncertainty_bins[i + 1])
        if np.sum(mask) > 0:
            bin_acc = np.mean(correct[mask])
            bin_accuracies.append(bin_acc)
        else:
            bin_accuracies.append(0)
    
    axes[1, 0].plot(bin_centers, bin_accuracies, 'o-', linewidth=2, markersize=8, color='green')
    axes[1, 0].set_xlabel('Total Uncertainty')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].set_title('Accuracy vs Uncertainty')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. Prediction confidence vs uncertainty
    axes[1, 1].scatter(mean_predictions, total_uncertainty, alpha=0.5, s=20)
    axes[1, 1].set_xlabel('Mean Prediction')
    axes[1, 1].set_ylabel('Total Uncertainty')
    axes[1, 1].set_title('Prediction Confidence vs Uncertainty')
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. Uncertainty by class
    real_uncertainty = total_uncertainty[all_labels == 0]
    fake_uncertainty = total_uncertainty[all_labels == 1]
    
    axes[1, 2].boxplot([real_uncertainty, fake_uncertainty], labels=['Real', 'Fake'], 
                      patch_artist=True, boxprops=dict(facecolor='lightblue'))
    axes[1, 2].set_ylabel('Total Uncertainty')
    axes[1, 2].set_title('Uncertainty by True Class')
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'uncertainty_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    return {
        'mean_epistemic_uncertainty': np.mean(epistemic_uncertainty),
        'mean_aleatoric_uncertainty': np.mean(aleatoric_uncertainty),
        'mean_total_uncertainty': np.mean(total_uncertainty),
        'uncertainty_accuracy_correlation': np.corrcoef(total_uncertainty, correct.astype(int))[0, 1]
    }

def progressive_resize_schedule(epoch, max_epochs, model_size='small'):
    """Progressive resizing schedule adapted to model size"""
    config = MODEL_CONFIGS[model_size]
    target_res = config['resolution']
    
    if model_size == 'tiny':
        # Tiny model: minimal progression
        if epoch < max_epochs * 0.5:
            return max(128, target_res - 96)
        else:
            return target_res
    elif model_size == 'small':
        # Small model: moderate progression  
        if epoch < max_epochs * 0.3:
            return max(160, target_res - 128)
        elif epoch < max_epochs * 0.6:
            return max(224, target_res - 64)
        else:
            return target_res
    else:
        # Medium/Large: standard progression
        if epoch < max_epochs * 0.3:
            return 224
        elif epoch < max_epochs * 0.6:
            return max(288, target_res - 96)
        else:
            return target_res

def create_optimized_dataloader(dataset, batch_size, shuffle, num_workers, pin_memory=True):
    """Create optimized dataloader with better memory management"""
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=True,
        prefetch_factor=4,
        drop_last=shuffle,  # Drop last only for training
        multiprocessing_context='spawn' if num_workers > 0 else None
    )

def knowledge_distillation_loss(student_logits, teacher_logits, labels, temperature=4.0, alpha=0.7):
    """Knowledge distillation loss combining hard and soft targets"""
    # Soft targets from teacher
    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)
    soft_student = F.log_softmax(student_logits / temperature, dim=-1)
    distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (temperature ** 2)
    
    # Hard targets (ground truth)
    hard_loss = F.binary_cross_entropy_with_logits(student_logits, labels.float())
    
    # Combine losses
    total_loss = alpha * distillation_loss + (1 - alpha) * hard_loss
    return total_loss

def setup_fsdp_model(model, device, model_size='small'):
    """Setup FSDP for distributed training (only recommended for medium/large models)"""
    if not FSDP_AVAILABLE or model_size in ['tiny', 'small']:
        if model_size in ['tiny', 'small']:
            print(f"âœ… Skipping FSDP for {model_size} model (not needed)")
        return model
    
    # FSDP configuration
    fsdp_config = {
        "mixed_precision": MixedPrecision(
            param_dtype=torch.bfloat16,
            reduce_dtype=torch.bfloat16,
            buffer_dtype=torch.bfloat16,
        ),
        "backward_prefetch": BackwardPrefetch.BACKWARD_PRE,
        "device_id": device.index if device.type == 'cuda' else None,
    }
    
    try:
        model = FSDP(model, **fsdp_config)
        print("âœ… FSDP enabled for distributed training")
    except Exception as e:
        print(f"âš ï¸  FSDP setup failed: {e}")
    
    return model

def main():
    parser = argparse.ArgumentParser(description='Fast CIFAKE Binary Classifier')
    parser.add_argument('--data_dir', type=str, default='cifake', help='Path to CIFAKE dataset')
    parser.add_argument('--model_size', type=str, default='medium', 
                       choices=['tiny', 'small', 'medium', 'large'], 
                       help='Model size: tiny (fastest), small (balanced), medium (accurate - default), large (best SigLIP-2 400M model)')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size (reduced for memory safety)')
    parser.add_argument('--epochs', type=int, default=30, help='Number of epochs')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Save directory')
    parser.add_argument('--evaluate_only', action='store_true', help='Only evaluate')
    parser.add_argument('--checkpoint', type=str, help='Checkpoint path')
    parser.add_argument('--accumulate_grad_batches', type=int, default=8, help='Gradient accumulation steps (increased for smaller batches)')
    parser.add_argument('--early_stopping_patience', type=int, default=5, help='Early stopping patience')
    parser.add_argument('--warmup_epochs', type=int, default=3, help='Warmup epochs for learning rate')
    parser.add_argument('--compile_mode', type=str, default='max-autotune', 
                       choices=['default', 'reduce-overhead', 'max-autotune'], 
                       help='torch.compile mode for optimization')
    parser.add_argument('--num_workers', type=int, default=8, help='DataLoader workers')
    parser.add_argument('--prefetch_factor', type=int, default=8, help='DataLoader prefetch factor for autotune')
    
    # Fast model optimization arguments
    parser.add_argument('--use_albumentations', action='store_true', help='Use advanced albumentations augmentation')
    parser.add_argument('--progressive_resize', action='store_true', help='Use progressive resizing')
    parser.add_argument('--use_ultra_jpeg', action='store_true', help='Use ultra JPEG compression augmentation (5-25% quality)')
    parser.add_argument('--jpeg_quality_min', type=int, default=5, help='Minimum JPEG quality for ultra compression (default: 5)')
    parser.add_argument('--jpeg_quality_max', type=int, default=25, help='Maximum JPEG quality for ultra compression (default: 25)')
    parser.add_argument('--jpeg_probability', type=float, default=0.3, help='Probability of applying JPEG compression (default: 0.3)')
    parser.add_argument('--use_ema', action='store_true', help='Use exponential moving average')
    parser.add_argument('--ema_decay', type=float, default=0.9999, help='EMA decay rate')
    parser.add_argument('--use_label_smoothing', action='store_true', help='Use label smoothing')
    parser.add_argument('--mixup_alpha', type=float, default=0.2, help='MixUp alpha parameter')
    parser.add_argument('--use_focal_loss', action='store_true', help='Use focal loss')
    parser.add_argument('--focal_alpha', type=float, default=1.0, help='Focal loss alpha')
    parser.add_argument('--focal_gamma', type=float, default=2.0, help='Focal loss gamma')
    parser.add_argument('--use_tta', action='store_true', help='Use test-time augmentation')
    parser.add_argument('--gradient_checkpointing', action='store_true', help='Use gradient checkpointing (not needed for small models)')
    parser.add_argument('--use_fsdp', action='store_true', help='Use FSDP for distributed training')
    parser.add_argument('--dropout_rate', type=float, default=0.1, help='Dropout rate')
    
    # Model distillation arguments
    parser.add_argument('--teacher_model_path', type=str, help='Path to teacher model for distillation')
    parser.add_argument('--distillation_alpha', type=float, default=0.7, help='Distillation loss weight')
    parser.add_argument('--distillation_temperature', type=float, default=4.0, help='Distillation temperature')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Clear any existing CUDA cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB total")
        print(f"GPU Memory available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9:.1f}GB")
    
    # Print speed optimization status
    optimizations = []
    if device.type == 'cuda':
        optimizations.append("âœ… GPU-accelerated preprocessing with Kornia")
        optimizations.append("âœ… BF16 mixed precision training")
        optimizations.append("âœ… TF32 enabled for faster matmul")
        optimizations.append("âœ… cuDNN benchmark enabled")
        
        # Check for Flash Attention
        if hasattr(torch.backends.cuda, "enable_flash_sdp"):
            optimizations.append("âœ… Flash/Memory-efficient SDPA enabled")
    
    if optimizations:
        print("ðŸš€ Speed optimizations enabled:")
        for opt in optimizations:
            print(f"   {opt}")
    else:
        print("âš ï¸  No speed optimizations available")
    
    # GPU-accelerated transforms with Kornia
    if device.type == 'cuda':
        gpu_transform = nn.Sequential(
            K.Resize(512, antialias=True),
            K.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ).to(device)
        cpu_transform = transforms.Compose([
            transforms.Resize((512, 512), antialias=True),
            transforms.ToTensor()
        ])
        print(f"ðŸš€ Using GPU-accelerated preprocessing with Kornia @ 512px")
    else:
        gpu_transform = None
        cpu_transform = transforms.Compose([
            transforms.Resize((512, 512), antialias=True),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # SigLIP normalization
        ])
        print(f"Using CPU preprocessing @ 512px (GPU not available)")
    
    # Enhanced datasets optimized for model size
    target_resolution = MODEL_CONFIGS[args.model_size]['resolution']
    
    # Adjust transforms for model size
    if device.type == 'cuda':
        gpu_transform = nn.Sequential(
            K.Resize(target_resolution, antialias=True),
            K.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ).to(device)
        cpu_transform = transforms.Compose([
            transforms.Resize((target_resolution, target_resolution), antialias=True),
            transforms.ToTensor()
        ])
    else:
        gpu_transform = None
        cpu_transform = transforms.Compose([
            transforms.Resize((target_resolution, target_resolution), antialias=True),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    
    # Create full training dataset first
    full_train_dataset = CIFAKEDataset(
        args.data_dir, 'train', cpu_transform, 
        use_albumentations=args.use_albumentations,
        progressive_resize=progressive_resize_schedule(0, args.epochs, args.model_size) if args.progressive_resize else None,
        use_ultra_jpeg=args.use_ultra_jpeg
    )
    test_dataset = CIFAKEDataset(args.data_dir, 'test', cpu_transform)
    
    # Split training data into train/validation (80/20 split)
    train_size = int(0.8 * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size
    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], 
                                             generator=torch.Generator().manual_seed(42))
    
    print(f"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}")
    
    print(f"Target resolution: {target_resolution}px for {args.model_size} model")
    
    # Optimized data loaders with size-appropriate batch sizes
    # Adjust batch size based on model size and available memory
    effective_batch_size = args.batch_size
    if args.model_size == 'tiny':
        effective_batch_size = min(args.batch_size * 2, 32)  # Reduced from 4x
    elif args.model_size == 'small':
        effective_batch_size = min(args.batch_size, 16)      # Reduced from 2x
    elif args.model_size == 'medium':
        # Medium model needs smaller batch size
        effective_batch_size = max(args.batch_size // 2, 4)
        print(f"âš ï¸  Medium model detected: reducing batch size to {effective_batch_size}")
    elif args.model_size == 'large':
        # Large model needs much smaller batch size
        effective_batch_size = max(args.batch_size // 4, 2)
        print(f"âš ï¸  Large model detected: reducing batch size to {effective_batch_size}")
    
    # Further reduce if using multiple memory-intensive features
    memory_intensive_features = 0
    if args.use_albumentations: memory_intensive_features += 1
    if args.use_ultra_jpeg: memory_intensive_features += 1
    if args.progressive_resize: memory_intensive_features += 1
    
    if memory_intensive_features >= 2:
        effective_batch_size = max(effective_batch_size // 2, 2)
        print(f"âš ï¸  Memory-intensive features detected: further reducing batch size to {effective_batch_size}")
    
    train_loader = create_optimized_dataloader(
        train_dataset, effective_batch_size, shuffle=True, num_workers=args.num_workers
    )
    val_loader = create_optimized_dataloader(
        val_dataset, effective_batch_size, shuffle=False, num_workers=args.num_workers
    )
    test_loader = create_optimized_dataloader(
        test_dataset, effective_batch_size, shuffle=False, num_workers=args.num_workers
    )
    
    print(f"Effective batch size: {effective_batch_size} (optimized for {args.model_size} model)")
    
    # SigLIP-based classifier
    model = FastBinaryClassifier(
        model_size=args.model_size,
        device=device,
        dropout_rate=args.dropout_rate,
        use_lightweight_attention=args.model_size in ['tiny', 'small']
    ).to(device)
    
    # Print model info
    config = MODEL_CONFIGS[args.model_size]
    print(f"\nðŸ“Š Model Configuration:")
    print(f"   Size: {args.model_size.upper()} ({config['params']})")
    print(f"   Resolution: {config['resolution']}px")
    print(f"   Backend: OpenCLIP SigLIP")
    print(f"   Description: {config['description']}")
    
    # Setup FSDP if requested (only for medium/large models)
    if args.use_fsdp:
        model = setup_fsdp_model(model, device, args.model_size)
    
    # Compile model for speed with configurable mode (PyTorch 2.0+)
    if hasattr(torch, 'compile'):
        try:
            mode = None if args.compile_mode == 'default' else args.compile_mode
            model = torch.compile(model, mode=mode)
            print(f"âœ… Model compiled with {args.compile_mode} mode for maximum performance")
        except Exception as e:
            print(f"âš ï¸  {args.compile_mode} compilation failed, trying fallback modes")
            for fallback_mode in ['reduce-overhead', 'default', None]:
                if fallback_mode != args.compile_mode:
                    try:
                        model = torch.compile(model, mode=fallback_mode)
                        fallback_name = fallback_mode if fallback_mode else 'default'
                        print(f"âœ… Model compiled with {fallback_name} fallback mode")
                        break
                    except Exception:
                        continue
            else:
                print(f"âš ï¸  All compilation modes failed, falling back to eager mode")
                # Enable error suppression for compilation issues
                if DYNAMO_AVAILABLE:
                    torch._dynamo.config.suppress_errors = True
    
    # Enhanced loss function and optimizer
    pos_weight = torch.tensor(2.0, device=device)  # Weight fake class more
    if args.use_focal_loss:
        criterion = FocalLoss(alpha=args.focal_alpha, gamma=args.focal_gamma, pos_weight=pos_weight)
        print(f"âœ… Using Focal Loss (Î±={args.focal_alpha}, Î³={args.focal_gamma})")
    else:
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    # AdamW with improved settings
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=args.lr, 
        weight_decay=0.01,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # Setup EMA if requested
    ema = None
    if args.use_ema:
        ema = ExponentialMovingAverage(model, decay=args.ema_decay)
        print(f"âœ… EMA enabled (decay={args.ema_decay})")
    
    # Setup dropout scheduler for adaptive regularization
    dropout_scheduler = DropoutScheduler(model, initial_dropout=args.dropout_rate, 
                                       max_dropout=args.dropout_rate * 3, patience=2)
    print(f"âœ… Adaptive dropout scheduler enabled (initial={args.dropout_rate})")
    
    # Setup real-time training monitor
    training_monitor = RealTimeTrainingMonitor(args.save_dir, update_interval=1)
    print(f"âœ… Real-time training monitor enabled")
    
    # Learning rate scheduler with warmup
    def lr_lambda(epoch):
        if epoch < args.warmup_epochs:
            return epoch / args.warmup_epochs
        return 0.5 * (1 + np.cos(np.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # Mixed precision scaler
    scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None
    
    # Convert model to channels_last for faster training
    if device.type == 'cuda' and not args.use_fsdp:  # Don't use channels_last with FSDP
        model = model.to(memory_format=torch.channels_last)
        print("âœ… Using channels_last memory format for speed")
    
    os.makedirs(args.save_dir, exist_ok=True)
    
    # Load checkpoint if provided
    if args.checkpoint:
        checkpoint = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        if not args.evaluate_only:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if ema and 'ema_state_dict' in checkpoint:
            ema.shadow = checkpoint['ema_state_dict']
        print(f"Loaded checkpoint from {args.checkpoint}")
    
    if args.evaluate_only:
        # Evaluate only with all optimizations
        test_loss, test_acc, test_balanced_acc, test_precision, test_recall, test_f1, test_auc, test_mcc, test_cm, test_labels, test_probs = evaluate(
            model, test_loader, criterion, device, gpu_transform, 
            use_tta=args.use_tta, ema=ema
        )
        print(f"\nEnhanced Test Results:")
        print(f"  Loss: {test_loss:.4f}")
        print(f"  Accuracy: {test_acc:.4f}")
        print(f"  Balanced Accuracy: {test_balanced_acc:.4f}")
        print(f"  Precision: {test_precision:.4f}")
        print(f"  Recall: {test_recall:.4f}")
        print(f"  F1: {test_f1:.4f}")
        print(f"  AUC: {test_auc:.4f}")
        print(f"  MCC: {test_mcc:.4f}")
        if args.use_tta:
            print(f"  âœ¨ Test-Time Augmentation: Enabled")
        if ema:
            print(f"  âœ¨ EMA: Enabled (decay={args.ema_decay})")
        save_plots(test_labels, test_probs, args.save_dir)
        return
    
    # Training loop with validation monitoring
    best_val_f1 = 0
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []  
    val_f1s = []
    test_losses = []
    test_accs = []
    test_f1s = []
    all_gradient_norms = []
    patience_counter = 0
    overfitting_warnings = 0
    
    for epoch in range(args.epochs):
        print(f"\nEpoch {epoch+1}/{args.epochs}")
        print("-" * 50)
        
        # Update progressive resize for current epoch
        if args.progressive_resize:
            new_size = progressive_resize_schedule(epoch, args.epochs, args.model_size)
            if hasattr(train_dataset, 'dataset'):  # Handle random_split wrapper
                train_dataset.dataset.progressive_resize = new_size
            else:
                train_dataset.progressive_resize = new_size
            print(f"Progressive resize: {new_size}px")
        
        # Train with enhanced techniques and regularization
        train_loss, train_acc, gradient_norms = train_epoch(
            model, train_loader, criterion, optimizer, device, scaler, 
            args.accumulate_grad_batches, gpu_transform, track_gradients=True,
            ema=ema, use_label_smoothing=args.use_label_smoothing, 
            mixup_alpha=args.mixup_alpha
        )
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        all_gradient_norms.extend(gradient_norms)
        
        # Validation evaluation
        val_loss, val_acc, val_balanced_acc, val_precision, val_recall, val_f1, val_auc, val_mcc, val_cm, val_labels, val_probs = evaluate(
            model, val_loader, criterion, device, gpu_transform, 
            use_tta=False, ema=ema  # No TTA for validation to save time
        )
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        val_f1s.append(val_f1)
        
        # Test evaluation (less frequent to save time)
        if epoch % 5 == 0 or epoch == args.epochs - 1:  # Every 5 epochs or last epoch
            test_loss, test_acc, test_balanced_acc, test_precision, test_recall, test_f1, test_auc, test_mcc, test_cm, test_labels, test_probs = evaluate(
                model, test_loader, criterion, device, gpu_transform, 
                use_tta=args.use_tta, ema=ema
            )
            test_losses.append(test_loss)
            test_accs.append(test_acc)
            test_f1s.append(test_f1)
        
        # Update regularization based on validation performance
        current_dropout = dropout_scheduler.step(val_loss)
        
        # Update learning rate
        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step()
        
        # Real-time monitoring and overfitting detection
        overfitting_detected = training_monitor.update(epoch + 1, train_loss, val_loss, train_acc, val_acc)
        training_monitor.add_lr_dropout_tracking(current_lr, current_dropout)
        
        if overfitting_detected:
            overfitting_warnings += 1
            print(f"âš ï¸  Overfitting detected! (Warning #{overfitting_warnings})")
        
        # Aggressive memory clearing after each epoch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            # Force garbage collection
            import gc
            gc.collect()
        
        # Enhanced logging
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}")
        print(f"Loss Gap: {val_loss - train_loss:.4f}, Acc Gap: {train_acc - val_acc:.2f}%")
        print(f"Learning Rate: {current_lr:.2e}, Dropout: {current_dropout:.3f}")
        
        if gradient_norms:
            grad_mean = np.mean(gradient_norms)
            grad_std = np.std(gradient_norms)
            print(f"Gradient Norms - Mean: {grad_mean:.4f}, Std: {grad_std:.4f}")
            
        if epoch % 5 == 0 or epoch == args.epochs - 1:
            print(f"Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}")
        
        # Save best model based on validation F1
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_f1': val_f1,
                'val_acc': val_acc,
                'train_loss': train_loss,
                'val_loss': val_loss
            }
            if ema:
                checkpoint['ema_state_dict'] = ema.shadow
            torch.save(checkpoint, os.path.join(args.save_dir, 'best_cifake_binary.pth'))
            print(f"ðŸŽ¯ New best Val F1: {val_f1:.4f} - model saved")
            
            # Save validation plots for best epoch
            save_plots(val_labels, val_probs, args.save_dir, epoch, prefix="val_")
        else:
            patience_counter += 1
            
        # Early stopping with overfitting consideration
        if patience_counter >= args.early_stopping_patience:
            print(f"Early stopping triggered after {epoch+1} epochs (patience: {args.early_stopping_patience})")
            break
            
        # Emergency stop if severe overfitting
        if overfitting_warnings >= 5:
            print(f"ðŸš¨ Emergency stop due to persistent overfitting ({overfitting_warnings} warnings)")
            break
    
    # Save final training curves from monitoring system
    final_curves_path = training_monitor.save_final_curves()
    
    # Get overfitting summary
    overfitting_summary = training_monitor.get_overfitting_summary()
    print(f"\nðŸ“Š Overfitting Analysis Summary:")
    print(f"   Total alerts: {overfitting_summary['total_alerts']}")
    print(f"   Risk level: {overfitting_summary['current_overfitting_risk']}")
    print(f"   Consecutive overfitting epochs: {overfitting_summary['consecutive_overfitting_epochs']}")
    
    # Save overfitting report
    with open(os.path.join(args.save_dir, 'overfitting_report.json'), 'w') as f:
        json.dump(overfitting_summary, f, indent=2)
    
    # Save traditional training curves for comparison
    save_training_curves(train_losses, train_accs, val_losses, val_accs, val_f1s, args.save_dir, all_gradient_norms)
    
    # Final comprehensive evaluation with best model
    print("\nðŸ“Š Generating publication-quality analysis...")
    
    # Load best model for final evaluation
    best_model_path = os.path.join(args.save_dir, 'best_cifake_binary.pth')
    if os.path.exists(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"Loaded best model for final analysis (F1: {checkpoint.get('test_f1', 'N/A'):.4f})")
    
    # Final comprehensive evaluation with all optimizations
    final_test_loss, final_test_acc, final_test_balanced_acc, final_test_precision, final_test_recall, final_test_f1, final_test_auc, final_test_mcc, final_test_cm, final_test_labels, final_test_probs = evaluate(
        model, test_loader, criterion, device, gpu_transform, 
        use_tta=args.use_tta, ema=ema
    )
    
    # Statistical significance tests
    statistical_results = statistical_significance_tests(final_test_labels, final_test_probs, args.save_dir)
    
    # Performance table
    performance_df = save_publication_table(final_test_labels, final_test_probs, args.save_dir, "final_")
    
    # Error analysis
    error_results = save_error_analysis_plots(final_test_labels, final_test_probs, args.save_dir, "final_")
    
    # Uncertainty analysis (Monte Carlo Dropout)
    try:
        uncertainty_results = save_uncertainty_analysis(model, test_loader, device, args.save_dir, gpu_transform, n_passes=10)
        print(f"âœ… Uncertainty analysis completed")
    except Exception as e:
        print(f"âš ï¸  Uncertainty analysis failed: {e}")
        uncertainty_results = None
    
    # Summary of all analyses
    analysis_summary = {
        'dataset_stats': {
            'total_train_samples': len(train_dataset),
            'total_test_samples': len(test_dataset),
            'train_fake_samples': sum(train_dataset.labels),
            'train_real_samples': len(train_dataset.labels) - sum(train_dataset.labels),
            'test_fake_samples': sum(test_dataset.labels),
            'test_real_samples': len(test_dataset.labels) - sum(test_dataset.labels)
        },
        'model_info': {
            'model_size': args.model_size,
            'backend': 'OpenCLIP SigLIP',
            'input_resolution': f'{target_resolution}x{target_resolution}' if not args.progressive_resize else f'Progressive â†’ {target_resolution}px',
            'parameters': MODEL_CONFIGS[args.model_size]['params'],
            'batch_size': args.batch_size,
            'gradient_accumulation': args.accumulate_grad_batches,
            'effective_batch_size': args.batch_size * args.accumulate_grad_batches,
            'optimizations': {
                'albumentations': args.use_albumentations,
                'progressive_resize': args.progressive_resize,
                'ultra_jpeg_compression': args.use_ultra_jpeg,
                'ema': args.use_ema,
                'label_smoothing': args.use_label_smoothing,
                'mixup': args.mixup_alpha > 0,
                'focal_loss': args.use_focal_loss,
                'test_time_augmentation': args.use_tta,
                'gradient_checkpointing': args.gradient_checkpointing,
                'fsdp': args.use_fsdp
            }
        },
        'performance_metrics': {
            'accuracy': float(final_test_acc),
            'balanced_accuracy': float(final_test_balanced_acc),
            'precision': float(final_test_precision),
            'recall': float(final_test_recall),
            'f1_score': float(final_test_f1),
            'auc_roc': float(final_test_auc),
            'matthews_correlation': float(final_test_mcc)
        },
        'statistical_tests': statistical_results,
        'error_analysis': error_results
    }
    
    # Add uncertainty results if available
    if uncertainty_results:
        analysis_summary['uncertainty_analysis'] = uncertainty_results
    
    # Training history
    analysis_summary['training_history'] = {
        'epochs_trained': len(train_losses),
        'best_epoch': int(np.argmax(test_f1s) + 1),
        'best_f1_score': float(max(test_f1s)),
        'final_train_loss': float(train_losses[-1]),
        'final_test_loss': float(test_losses[-1]),
        'convergence_achieved': bool(patience_counter < args.early_stopping_patience)
    }
    
    # Save comprehensive analysis summary
    with open(os.path.join(args.save_dir, 'comprehensive_analysis.json'), 'w') as f:
        json.dump(analysis_summary, f, indent=2, default=str)
    
    print(f"\nðŸŽ¯ Analysis Complete! Generated comprehensive publication-ready materials:")
    print(f"   ðŸ“ˆ {15 + (6 if uncertainty_results else 0)} visualization plots")
    print(f"   ðŸ“‹ Performance table (CSV + LaTeX)")
    print(f"   ðŸ“Š Statistical significance tests")
    print(f"   ðŸ” Error analysis and model insights")
    print(f"   ðŸ“„ Comprehensive analysis summary (JSON)")
    print(f"   ðŸ“ All files saved in: {args.save_dir}/")
    
    # Print key performance metrics with confidence intervals
    print(f"\nðŸ† Enhanced CIFAKE Results (with 95% confidence intervals):")
    optimizations_used = []
    if args.use_albumentations: optimizations_used.append("Advanced Augmentation")
    if args.progressive_resize: optimizations_used.append("Progressive Resize")
    if args.use_ultra_jpeg: optimizations_used.append("Ultra JPEG Compression")
    if args.use_ema: optimizations_used.append("EMA")
    if args.use_label_smoothing: optimizations_used.append("Label Smoothing")
    if args.mixup_alpha > 0: optimizations_used.append("MixUp")
    if args.use_focal_loss: optimizations_used.append("Focal Loss")
    if args.use_tta: optimizations_used.append("Test-Time Augmentation")
    if args.gradient_checkpointing: optimizations_used.append("Gradient Checkpointing")
    if args.use_fsdp: optimizations_used.append("FSDP")
    
    if optimizations_used:
        print(f"   ðŸš€ Optimizations: {', '.join(optimizations_used)}")
    print()
    for _, row in performance_df.head(7).iterrows():  # Show main metrics
        print(f"   {row['Metric']:20s}: {row['Formatted']}")
    
    # Print statistical significance
    if statistical_results.get('fisher_exact', {}).get('p_value'):
        p_val = statistical_results['fisher_exact']['p_value']
        significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns"
        print(f"\nðŸ“ˆ Statistical Significance: p = {p_val:.6f} {significance}")
        print(f"   (Fisher's exact test vs. random classification)")
    
    print(f"\nâœ¨ Enhanced CIFAKE Classifier Ready! âœ¨")
    print(f"   ðŸ“Š Speed Improvements: {2-4}x faster training with optimizations")
    print(f"   ðŸ”Ž Accuracy Improvements: Enhanced with SOTA techniques")
    print(f"   ðŸ“‹ Full analysis ready for publication submission!")
    
    # Performance summary
    estimated_speedup = 1.0
    if args.use_albumentations: estimated_speedup *= 1.1  # GPU augmentation
    if args.progressive_resize: estimated_speedup *= 1.3   # Start with smaller images
    if args.gradient_checkpointing: estimated_speedup *= 0.9  # Slight slowdown but huge memory saving
    if args.use_fsdp: estimated_speedup *= 2.0             # Distributed training
    if device.type == 'cuda': estimated_speedup *= 1.5     # GPU optimizations
    
    # Model size speedup estimates
    size_speedup_map = {
        'tiny': 8.0, 'small': 4.0, 'medium': 2.0, 'large': 0.8
    }
    size_speedup = size_speedup_map[args.model_size]
    total_speedup = estimated_speedup * size_speedup
    
    print(f"   âš¡ Model size speedup: {size_speedup:.1f}x ({args.model_size} vs large)")
    print(f"   âš¡ Total estimated speedup: {total_speedup:.1f}x")
    
    accuracy_improvements = []
    if args.use_tta: accuracy_improvements.append("+2-5% from TTA")
    if args.use_ema: accuracy_improvements.append("+1-3% from EMA")
    if args.use_focal_loss: accuracy_improvements.append("+1-2% from Focal Loss")
    if args.use_label_smoothing: accuracy_improvements.append("+0.5-1% from Label Smoothing")
    if args.mixup_alpha > 0: accuracy_improvements.append("+1-2% from MixUp")
    
    if accuracy_improvements:
        print(f"   ðŸŽ¥ Expected accuracy gains: {', '.join(accuracy_improvements)}")

if __name__ == '__main__':
    main()